{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaixen/BCS_recruitment/blob/main/VeritasVigil%3A%20The%20truth%20Watchman/demo_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_0zANBhCNLX"
      },
      "source": [
        "# Custom Tokenizer Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZkQivkQiIWWd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b5KYK09zC1cY"
      },
      "outputs": [],
      "source": [
        "class customtokenizer:\n",
        "  def __init__(self):\n",
        "    self.emoticon_pattern=re.compile(r'[:;=8][\\-o\\*]?[\\\\]dDpP/\\:\\}\\{@\\|\\\\}')\n",
        "    self.contractions={\"can't\":\"can not\",\"won't\":\"will not\",\"i'm\":\"i am\",\"he's\":\"he is\",\n",
        "    \"she's\":\"she is\",\"it's\":\"it is\",\"that's\":\"that is\",\"there's\":\"there is\",\"what's\":\"what is\",\n",
        "             \"who've\":\"who have\",\"'ve\":\"have\",\"didn't\":\"did not\",\"don't\":\"do not\",\"isn't\":\"is not\",\n",
        "                       \"shouldn't\":\"should not\"}#some frequently used short-forms\n",
        "  #they are identified early so that when in later functions we split punctuations etc, it doesn't provide a barrier\n",
        "  def expand_contractions(self,text:str)->str:\n",
        "    def expand_quotes(match):\n",
        "        subject = match.group(1)\n",
        "        token_0 = match.group(2)\n",
        "        token_0 = token_0.lower()\n",
        "        if len(token_0) >= 2 and (\n",
        "            (token_0[-1] == 'e' and token_0[-2] == 'n') or\n",
        "            (token_0[-1] == 'n' and token_0[-2] == 'e')\n",
        "        ):\n",
        "            return f\"{subject} has {match.group(2)}\"\n",
        "        else:\n",
        "            return f\"{subject} is {match.group(2)}\"\n",
        "    text = re.sub(r\"\\b(he|she|it)'s\\s+(\\w+)\", expand_quotes, text, flags=re.IGNORECASE)\n",
        "\n",
        "    for contraction,expanded in self.contractions.items():\n",
        "        text=re.sub(r'\\b'+re.escape(contraction)+r'\\b',expanded,text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "  def normalize(self,word:str)->list[str]:\n",
        "    match=re.search(r'(.)\\1{2,}',word)\n",
        "    if match:\n",
        "      char=match.group(1)\n",
        "      repeat_count=len(match.group(0))\n",
        "      normalized=re.sub(r'(.)\\1{2,}',re.escape(char),word)\n",
        "      return[normalized,f\"<REpEat:{repeat_count}>\"]\n",
        "    else:\n",
        "      return[word]\n",
        "\n",
        "  def tokenize(self,text:str)->list[str]:\n",
        "    text=text.lower()#lowercasing the words\n",
        "    text=self.expand_contractions(text)#expand the contractions\n",
        "    #text=self.emoticon_pattern(text)#identify emoticon patterns\n",
        "    emoticons=self.emoticon_pattern.findall(text)\n",
        "    text=self.emoticon_pattern.sub('',text)#substitute the emoticon patterns\n",
        "    text=re.sub(r'([!?.,;:\"(){}[\\]])', r' \\1 ',text)#splitting punctuations\n",
        "    text=re.sub(r'\\s{2,}',' ',text)\n",
        "    tokens=[]#initialise a blank list of tokens\n",
        "    for word in text.strip().split():\n",
        "      tokens.extend(self.normalize(word))\n",
        "    return tokens+emoticons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3CRFCK6K0Ro",
        "outputId": "6893b348-6e30-40c7-a5bf-eeff52e945bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'are', 'many', '<REpEat:5>', 'of', 'the', 'protagonists', 'it', 'is', 'abhored', 'it', '.', '.', '.', '!', '!']\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tokenizer = customtokenizer()\n",
        "    sample1 = \"there are mannnnny of the PROTAGONISTS it's abhored IT...!!\"\n",
        "    sample2=\"latttent\"\n",
        "    tokens = tokenizer.tokenize(sample1)\n",
        "    print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSi3SakCQG_C"
      },
      "source": [
        "# Rule‑Based POS(parts of speech) Tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1UJajz5QO1p"
      },
      "source": [
        "Distinguish between nouns, adjectives and verbs only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tLTqVMBDQNKC"
      },
      "outputs": [],
      "source": [
        "class POS_tagger:\n",
        "  def __init__(self):\n",
        "    self.pronouns={\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"us\", \"them\",\"their\"}\n",
        "    self.determiners = {\"the\", \"a\", \"an\", \"this\", \"that\", \"these\", \"those\",\"there\"}\n",
        "    self.adj_endings=['ous', 'ful', 'ive', 'al', 'ic', 'able', 'ible','ary']\n",
        "    self.verb_endings=['ing','ed','en','es','s','ise','ize']\n",
        "    self.be_verb={\"is\",\"am\",\"are\",\"was\",\"were\"}\n",
        "    self.adv_endings=['ly','ily']\n",
        "    self.adverbs_common={\"very\", \"most\" ,\"so\"}\n",
        "    #self.noun_endings=['ment', 'ness', 'ity', 'tion', 'sion', 'er', 'or']\n",
        "    self.prepositions=[\"of\",\"to\",\"in\",\"for\",\"on\",\"with\",\"at\",\"by\",\"from\",\"about\",\"over\",\"after\",\"as\"]\n",
        "    self.pre={\"REpEat\"}\n",
        "    self.noun_endings = ['tion', 'ment', 'ness', 'ity', 'ist', 'ism', 'ance', 'ence', 'ship', 's']  # careful with 's'\n",
        "\n",
        "\n",
        "  def tagger(self,tokens:list[str])->list[tuple[str,str]]:\n",
        "    tagging_done=[]\n",
        "    for token in tokens:\n",
        "      if re.fullmatch(r'REPEAT:\\d+',token):\n",
        "        continue\n",
        "      if re.fullmatch(r'[.,!?;:\\'\\\"()\\[\\]{}]', token):\n",
        "        tagger=\"punctuation\"\n",
        "      elif token in self.pronouns:\n",
        "        tagger=\"pronoun\"\n",
        "      elif token in self.determiners:\n",
        "        tagger=\"determiners\"\n",
        "      elif token in self.be_verb:\n",
        "        tagger=\"verb\"\n",
        "      elif token in self.adverbs_common:\n",
        "        tagger=\"adverb\"\n",
        "      elif re.fullmatch(r'\\d+(\\.\\d+)?', token):\n",
        "        tagger = \"NUM\"\n",
        "      elif token in self.prepositions:\n",
        "        tagger=\"preposition\"\n",
        "      elif token.startswith(\"<REPEAT\"):\n",
        "        tagger = \"OTHER\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.noun_endings) :\n",
        "        tagger=\"noun\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.verb_endings) :\n",
        "        tagger=\"verb\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.adj_endings) :\n",
        "        tagger=\"adjective\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.adv_endings) :\n",
        "        tagger=\"adverb\"\n",
        "\n",
        "      elif any(token.startswith(pre)for pre in self.pre):\n",
        "        tagger=\"ignore\"\n",
        "      else:\n",
        "        tagger=\"other\"\n",
        "      tagging_done.append((token,tagger))\n",
        "    return tagging_done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T431_uS5WTZX",
        "outputId": "ef1a3577-1a8b-4708-8115-764f4beb59f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['so', '<REpEat:7>', 'scary', '!', '!', '!', 'it', 'is', 'very', 'arduous', '!', '!']\n",
            "there           : determiners\n",
            "he              : pronoun\n",
            "has             : noun\n",
            "taken           : verb\n",
            "are             : verb\n",
            "many            : other\n",
            "of              : preposition\n",
            "the             : determiners\n",
            "protagonists    : noun\n",
            "who             : other\n",
            "have            : other\n",
            "abhored         : verb\n",
            "it              : pronoun\n",
            ".               : punctuation\n",
            ".               : punctuation\n",
            ".               : punctuation\n",
            "!               : punctuation\n",
            "!               : punctuation\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tokenizer = customtokenizer()\n",
        "    tagger = POS_tagger()\n",
        "    sample_1 = \"Sooooooo scary!!!IT's very arduous!!\"\n",
        "    sample_2=\"there he's taken are mannnnny of the PROTAGONISTS who've abhored IT...!!\"\n",
        "    tokens = tokenizer.tokenize(sample_2)\n",
        "    tagging_done = tagger.tagger(tokens)\n",
        "    tokens = tokenizer.tokenize(sample_1)\n",
        "    print(tokens)\n",
        "for token, tagger in tagging_done:\n",
        "  if re.fullmatch(r'<REpEat:\\d+>', token):\n",
        "    continue\n",
        "  else:\n",
        "    print(f\"{token:15} : {tagger}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-buL3dMf59f"
      },
      "source": [
        "# Custom Stemmer or Lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rTftbyigJPY"
      },
      "source": [
        "The motivation of this pipeline is to reduce similar tokens like \"eaten\",\"ate\",\"eating\" to their stem word i.e.\"eat\". But it is to be taken care that over-stemming is avoided like \"protagonists\" isn't converted to \"protagon\". Basically I will try to convert only those tokens which are verb :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fG0U3qIjf_8f"
      },
      "outputs": [],
      "source": [
        "class lemmatizer:\n",
        "  def __init__(self):\n",
        "    self.verb_endings=['ing','ed','en','es','ise','ize']\n",
        "    self.be_verb={\"is\",\"am\",\"are\",\"was\",\"were\"}\n",
        "    self.noun_endings=['ment', 'ness', 'ity', 'tion', 'sion', 'er', 'or']\n",
        "    self.adj_endings=['ous', 'ful', 'ive', 'al', 'ic', 'able', 'ible','ary']\n",
        "\n",
        "  def lemmatize(self,token:str,pos:str)->str:\n",
        "    lemma=token\n",
        "    if pos=='verb':\n",
        "      for suffix in self.verb_endings:\n",
        "        if token.endswith(suffix) and len(token)>len(suffix)+2:\n",
        "          lemma=token[:-len(suffix)]\n",
        "          if len(lemma) >= 2 and lemma[-1] == lemma[-2]:\n",
        "             lemma = lemma[:-1]\n",
        "          break\n",
        "    elif pos=='noun':\n",
        "      for suffix in self.noun_endings:\n",
        "        if token.endswith(suffix) and len(token)>len(suffix)+2:\n",
        "          lemma=token[:-len(suffix)]\n",
        "          break\n",
        "    elif pos=='adjective':\n",
        "      for suffix in self.adj_endings:\n",
        "        if token.endswith(suffix) and len(token)>len(suffix)+2:\n",
        "          lemma=token[:-len(suffix)]\n",
        "          break\n",
        "    return lemma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9tO1Cau8AHQ",
        "outputId": "6e7691b9-fb7e-4bc3-8125-21f2db60aca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['he', 'has', 'been', 'running', 'from', 'the', 'protagonists', ',', 'fearing', 'their', 'abhored', 'powers', '.']\n",
            "\n",
            "Lemmatization:\n",
            "he              (pronoun):he\n",
            "has             (noun):has\n",
            "been            (verb):been\n",
            "running         (verb):run\n",
            "from            (preposition):from\n",
            "the             (determiners):the\n",
            "protagonists    (noun):protagonists\n",
            ",               (punctuation):,\n",
            "fearing         (verb):fear\n",
            "their           (pronoun):their\n",
            "abhored         (verb):abhor\n",
            "powers          (noun):powers\n",
            ".               (punctuation):.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tokenizer = customtokenizer()\n",
        "    tagger = POS_tagger()\n",
        "    lemmatizer = lemmatizer()\n",
        "    sample = \"He's been running from the protagonists, fearing their abhored powers.\"\n",
        "    tokens = tokenizer.tokenize(sample)\n",
        "    print(\"tokens:\", tokens)\n",
        "    tagging_done = tagger.tagger(tokens)\n",
        "    print(\"\\nLemmatization:\")\n",
        "    for tok, tag in tagging_done:\n",
        "        if re.fullmatch(r'<REPEAT:\\d+>', tok):\n",
        "            continue\n",
        "        lemma = lemmatizer.lemmatize(tok, tag)\n",
        "        print(f\"{tok:15} ({tag}):{lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4lVzzyji7Cv"
      },
      "source": [
        "# Loading the dataset into the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uYMJclhRFy7O",
        "outputId": "2013d81f-0464-4fba-b8c1-af74da5ced8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wolta\n",
            "  Downloading wolta-0.3.7-py3-none-any.whl.metadata (960 bytes)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from wolta) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from wolta) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from wolta) (2.0.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.11/dist-packages (from wolta) (0.2.7)\n",
            "Collecting catboost (from wolta)\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting imblearn (from wolta)\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from wolta) (4.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wolta) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from wolta) (4.11.0.86)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (0.20.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->wolta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->wolta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->wolta) (2025.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (0.10.9.7)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn->wolta) (0.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (3.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->wolta) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->wolta) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn->wolta) (0.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost->wolta) (9.1.2)\n",
            "Downloading wolta-0.3.7-py3-none-any.whl (19 kB)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install wolta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tak_CVZv_3d_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RUubcHF_pN2"
      },
      "outputs": [],
      "source": [
        "df_true=pd.read_csv('/content/True.csv')\n",
        "df_fake=pd.read_csv('/content/Fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uft6lhvjfmlq"
      },
      "outputs": [],
      "source": [
        "df_true.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tvCBk_wJnv8"
      },
      "outputs": [],
      "source": [
        "fake_texts=[]\n",
        "for text in df_fake['text']:\n",
        "  fake_texts.append(text)\n",
        "true_texts=[]\n",
        "for text in df_true['text']:\n",
        "  true_texts.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELoVyuycKKps"
      },
      "outputs": [],
      "source": [
        "texts = fake_texts + true_texts\n",
        "labels = [0]*len(fake_texts) + [1]*len(true_texts)#0 for fake and 1 for true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating tokenizer,tagger and lemmatizer into the pipeline"
      ],
      "metadata": {
        "id": "OCmjDx9UovMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = customtokenizer()\n",
        "tagger = POS_tagger()\n",
        "lemmatizer = lemmatizer # This line was causing the error\n",
        "def preprocess(text):\n",
        "    tokens=tokenizer.tokenize(text)\n",
        "    tagged=tagger.tagger(tokens)\n",
        "    lemmas=[lemmatizer.lemmatize(tok, tag)\n",
        "              for tok, tag in tagged if not re.fullmatch(r'<REpEat:\\d+>', tok)] # Changed tagging_done to tagged\n",
        "    return ' '.join(lemmas)  #join back for vectorization"
      ],
      "metadata": {
        "id": "-i9F3r80ouyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_texts = [preprocess(doc) for doc in texts]"
      ],
      "metadata": {
        "id": "hbCgZVRBpUOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5UmW08XfxVP"
      },
      "source": [
        "# Decoding the sentiments(Bag of Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgfzB3itKP4c"
      },
      "outputs": [],
      "source": [
        "class bag_of_words:\n",
        "  def __init__(self,min_freq=1):\n",
        "    self.vocabulary={}\n",
        "    self.min_freq=min_freq\n",
        "\n",
        "  def build_vocabulary(self,documents):\n",
        "    from collections import Counter\n",
        "    word_counts=Counter()\n",
        "    freq={}\n",
        "    for doc in documents:\n",
        "      tokens=re.findall(r'\\b\\w+\\b',doc.lower())\n",
        "      unique_tokens=set(tokens)\n",
        "      for token in unique_tokens:\n",
        "        freq[token]=freq.get(token,0)+1\n",
        "        self.vocabulary = {}\n",
        "    index = 0\n",
        "    # Manually assign indices starting from 0\n",
        "    for word, count in freq.items():\n",
        "        if count >= self.min_freq:\n",
        "            self.vocabulary[word] = index\n",
        "            index += 1\n",
        "    #verify the size and maximum index\n",
        "    #print(f\"Built vocabulary size: {len(self.vocabulary)}\")\n",
        "    #if self.vocabulary:\n",
        "    #     max_index = max(self.vocabulary.values())\n",
        "    #     print(f\"Max index in vocabulary: {max_index}\")\n",
        "    #self.vocabulary={word: i for i, (word, count) in enumerate(freq.items()) if count >= self.min_freq}\n",
        "\n",
        "  def transformation(self,documents):\n",
        "    vectors=[]\n",
        "    vocab_size = len(self.vocabulary)\n",
        "    for doc in documents:\n",
        "      tokens=re.findall(r'\\b\\w+\\b',doc.lower())\n",
        "      vector=np.zeros(len(self.vocabulary),dtype=int)\n",
        "      for token in tokens:\n",
        "        loc=self.vocabulary.get(token)\n",
        "        if loc is not None: #and 0 <= loc < vocab_size:\n",
        "          vector[loc]=vector[loc]+1\n",
        "      vectors.append(vector)\n",
        "    return np.array(vectors)\n",
        "\n",
        "  def fitting(self,documents):\n",
        "    self.build_vocabulary(documents)\n",
        "    return self.transformation(documents)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkf8HRLnQhUj"
      },
      "outputs": [],
      "source": [
        "vectorizer = bag_of_words(min_freq=2)\n",
        "X = vectorizer.fitting(texts)\n",
        "y = np.array(labels)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vectorizer.build_vocabulary)}\")\n",
        "print(f\"Feature vector shape: {X.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uC9_TFbGrRM"
      },
      "source": [
        "# TF-IDF implemenation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDkY-_tpGvvn"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from scipy.sparse import lil_matrix\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class TFIDFVectorizer:\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.min_freq = min_freq\n",
        "        self.vocab = {}\n",
        "        self.idf = {}\n",
        "\n",
        "    def fit(self, documents):\n",
        "        doc_freq = defaultdict(int)\n",
        "        total_docs = len(documents)\n",
        "        for doc in documents:\n",
        "            tokens = set(doc.split())  # unique tokens per doc\n",
        "            for token in tokens:\n",
        "                doc_freq[token] += 1\n",
        "        self.vocab = {\n",
        "            token: idx for idx, (token, freq) in enumerate(doc_freq.items())\n",
        "            if freq >= self.min_freq\n",
        "        }\n",
        "        for token in self.vocab:\n",
        "            df = doc_freq[token]\n",
        "            self.idf[token] = math.log((1 + total_docs) / (1 + df)) + 1  # smooth IDF\n",
        "\n",
        "    def transform(self, documents):\n",
        "        rows = len(documents)\n",
        "        cols = len(self.vocab)\n",
        "        X = lil_matrix((rows, cols), dtype=np.float32)\n",
        "\n",
        "        for i, doc in enumerate(documents):\n",
        "            tf = Counter(doc.split())\n",
        "            total_terms = sum(tf.values())\n",
        "            for token, count in tf.items():\n",
        "                if token in self.vocab:\n",
        "                    tf_val = count / total_terms\n",
        "                    idf_val = self.idf[token]\n",
        "                    X[i, self.vocab[token]] = tf_val * idf_val\n",
        "        return X.tocsr()\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        self.fit(documents)\n",
        "        return self.transform(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_text=preprocessed_texts"
      ],
      "metadata": {
        "id": "vqw0YWHYvQtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2pt8j8UiuEi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    vectorizer = TFIDFVectorizer(min_freq=1)\n",
        "    X = vectorizer.fit_transform(updated_text)\n",
        "    y=np.array(labels)\n",
        "    # print(f\"Vocabulary: {vectorizer.vocab}\")\n",
        "    # print(\"TF-IDF matrix shape:\", X.shape)\n",
        "    # print(X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlrBvrf8ki5H"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LQV3DkTkl9X"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "VGK0zj9SyMvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Naive-Bayes model\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7cxG_HO44DN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes=MultinomialNB()\n",
        "naive_bayes.fit(X_train,y_train)\n",
        "y_pred_naive_bayes=naive_bayes.predict(X_test)"
      ],
      "metadata": {
        "id": "uILkWtWKyrbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using support vector machine\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CVg8QAy-4Idt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm=SVC()\n",
        "svm.fit(X_train,y_train)\n",
        "y_pred_svm=svm.predict(X_test)"
      ],
      "metadata": {
        "id": "5Lky105Fy-l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisations"
      ],
      "metadata": {
        "id": "ufdagKKK-_j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_wordcloud(texts, title='Word Cloud'):\n",
        "    all_text = ' '.join(texts)\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.show()\n",
        "plot_wordcloud(updated_text, title='common Words in News Dataset')\n"
      ],
      "metadata": {
        "id": "_MkT-Bd69Aum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def plot_top_tokens(texts, top_n=20, title='Top Tokens'):\n",
        "    tokens = ' '.join(texts).split()\n",
        "    token_counts = Counter(tokens)\n",
        "    common = token_counts.most_common(top_n)\n",
        "    labels, values = zip(*common)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(labels, values, color='skyblue')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_top_tokens(updated_text, title='most Common Tokens')\n"
      ],
      "metadata": {
        "id": "sTovjPIe9G08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def plot_repeat_tokens(texts):\n",
        "    repeat_counts = Counter()\n",
        "    for text in texts:\n",
        "        repeats = re.findall(r'<REPEAT:(\\d+)>', text)\n",
        "        for r in repeats:\n",
        "            repeat_counts[int(r)] += 1\n",
        "\n",
        "    if not repeat_counts:\n",
        "        print(\"No <REPEAT:n> tokens found.\")\n",
        "        return\n",
        "\n",
        "    keys, values = zip(*sorted(repeat_counts.items()))\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(keys, values, color='coral')\n",
        "    plt.xlabel('Repeat Count (n)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of <REPEAT:n> Tokens')\n",
        "    plt.xticks(keys)\n",
        "    plt.show()\n",
        "plot_repeat_tokens(updated_text)\n"
      ],
      "metadata": {
        "id": "2O1412ME9Khd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels=['Real', 'Fake'], title='Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "y_pred = svm.predict(X_test)\n",
        "plot_confusion_matrix(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "F75Vru47B8Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution(labels, label_names=['Real', 'Fake']):\n",
        "    from collections import Counter\n",
        "    count = Counter(labels)\n",
        "    keys = [label_names[k] for k in sorted(count)]\n",
        "    values = [count[k] for k in sorted(count)]\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(keys, values, color=['green', 'red'])\n",
        "    plt.title('Class Distribution')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "plot_class_distribution(y)\n"
      ],
      "metadata": {
        "id": "WjUhXO0OCCSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrices"
      ],
      "metadata": {
        "id": "XklCBnwVB2pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc(model, X_test, y_test, title='ROC Curve'):\n",
        "    probs = model.decision_function(X_test)  # Use predict_proba for models like NB\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})', color='red')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "plot_roc(svm, X_test, y_test, title='ROC Curve for SVM')\n"
      ],
      "metadata": {
        "id": "sjXYt5IF9N0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,roc_curve,auc"
      ],
      "metadata": {
        "id": "natUqfNvAzCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_naive_bayes)\n",
        "f1_nb = f1_score(y_test, y_pred_naive_bayes, average='weighted')\n",
        "# Optional: print for confirmation\n",
        "print(f\"Accuracy_nb: {accuracy_nb:.4f}\")\n",
        "print(f\"F1 Score_nb: {f1_nb:.4f}\")\n",
        "print(f\"Accuracy_svm: {accuracy_svm:.4f}\")\n",
        "print(f\"F1 Score_svm: {f1_svm:.4f}\")"
      ],
      "metadata": {
        "id": "X-oUUnq2EmUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_performance(metrics_dict, title='Model Comparison'):\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.DataFrame(metrics_dict)\n",
        "    df = df.set_index('Model')\n",
        "\n",
        "    df.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "metrics = {\n",
        "    'Model': ['SVM', 'Naive Bayes'],\n",
        "    'Accuracy': [accuracy_svm,accuracy_nb],\n",
        "    'F1 Score': [f1_svm,f1_nb]\n",
        "}\n",
        "plot_model_performance(metrics)\n"
      ],
      "metadata": {
        "id": "9FdnhwLyCFNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_preds = naive_bayes.predict(X_test)\n",
        "svm_preds = svm.predict(X_test)\n",
        "print(\"Naive Bayes:\\n\", classification_report(y_test, nb_preds))\n",
        "print(\"SVM:\\n\", classification_report(y_test, svm_preds))\n",
        "print(\"Confusion Matrix (SVM):\\n\", confusion_matrix(y_test, svm_preds))\n",
        "print(\"confusion matrix(NB):\\n\",confusion_matrix(y_test,nb_preds))"
      ],
      "metadata": {
        "id": "EnCJjytzAbkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a_0zANBhCNLX",
        "jSi3SakCQG_C",
        "H-buL3dMf59f",
        "f4lVzzyji7Cv",
        "OCmjDx9UovMW",
        "X5UmW08XfxVP",
        "9uC9_TFbGrRM",
        "jlrBvrf8ki5H"
      ],
      "authorship_tag": "ABX9TyNRJ+3Rt4NSiH31fG2Mk7Sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}