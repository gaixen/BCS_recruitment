{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNYAk4P15VBnu/HBZcxc6p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaixen/BCS_recruitment/blob/main/VeritasVigil%3A%20The%20truth%20Watchman/demo_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding the sentiments"
      ],
      "metadata": {
        "id": "X5UmW08XfxVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "uYMJclhRFy7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d630f673-9e4c-4d38-a688-4de8b537dc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wolta in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from wolta) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from wolta) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from wolta) (2.0.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.11/dist-packages (from wolta) (0.2.7)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (from wolta) (1.2.8)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (from wolta) (0.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from wolta) (4.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wolta) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from wolta) (4.11.0.86)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (0.20.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost->wolta) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->wolta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->wolta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->wolta) (2025.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt->wolta) (0.10.9.7)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn->wolta) (0.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wolta) (3.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->wolta) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->wolta) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn->wolta) (0.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost->wolta) (9.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wolta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from typing import Text"
      ],
      "metadata": {
        "id": "tak_CVZv_3d_"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true=pd.read_csv('/content/True.csv')\n",
        "df_fake=pd.read_csv('/content/Fake.csv')"
      ],
      "metadata": {
        "id": "5RUubcHF_pN2"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Tokenizer Development"
      ],
      "metadata": {
        "id": "a_0zANBhCNLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customtokenizer:\n",
        "  def __init__(self):\n",
        "    self.emoticon_pattern=re.compile(r'[:;=8][\\-o\\*]?[\\\\]dDpP/\\:\\}\\{@\\|\\\\}')\n",
        "    self.contractions={\"can't\":\"can not\",\"won't\":\"will not\",\"i'm\":\"i am\",\"he's\":\"he is\",\n",
        "    \"she's\":\"she is\",\"it's\":\"it is\",\"that's\":\"that is\",\"there's\":\"there is\",\"what's\":\"what is\",\n",
        "             \"who've\":\"who have\",\"'ve\":\"have\",\"didn't\":\"did not\",\"don't\":\"do not\",\"isn't\":\"is not\",\n",
        "                       \"shouldn't\":\"should not\"}#some frequently used short-forms\n",
        "  #they are identified early so that when in later functions we split punctuations etc, it doesn't provide a barrier\n",
        "  def expand_contractions(self,text:str)->str:\n",
        "      for contraction,expanded in self.contractions.items():\n",
        "        text=re.sub(r'\\b'+re.escape(contraction)+r'\\b',expanded,text)\n",
        "\n",
        "      return text\n",
        "\n",
        "\n",
        "  def normalize(self,word:str)->list[str]:\n",
        "    match=re.search(r'(.)\\1{2,}',word)\n",
        "    if match:\n",
        "      char=match.group(1)\n",
        "      repeat_count=len(match.group(0))\n",
        "      normalized=re.sub(r'(.)\\1{2,}',char,word)\n",
        "      return[normalized,f\"<REpEat:{repeat_count}>\"]\n",
        "    else:\n",
        "      return[word]\n",
        "\n",
        "  def tokenize(self,text:str)->list[str]:\n",
        "    text=text.lower()#lowercasing the words\n",
        "    text=self.expand_contractions(text)#expand the contractions\n",
        "    #text=self.emoticon_pattern(text)#identify emoticon patterns\n",
        "    emoticons=self.emoticon_pattern.findall(text)\n",
        "    text=self.emoticon_pattern.sub('',text)#substitute the emoticon patterns\n",
        "    text=re.sub(r'([!?.,;:\"(){}[\\]])', r' \\1 ',text)#splitting punctuations\n",
        "    text=re.sub(r'\\s{2,}',' ',text)\n",
        "    tokens=[]#initialise a blank list of tokens\n",
        "    for word in text.strip().split():\n",
        "      tokens.extend(self.normalize(word))\n",
        "    return tokens+emoticons\n"
      ],
      "metadata": {
        "id": "b5KYK09zC1cY"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tokenizer = customtokenizer()\n",
        "    sample1 = \"there are mannnnny of the PROTAGONISTS it's abhored IT...!!\"\n",
        "    sample2=\"latttent\"\n",
        "    tokens = tokenizer.tokenize(sample1)\n",
        "    print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3CRFCK6K0Ro",
        "outputId": "ffc4d125-8dc9-4e2e-b577-86cc218ef4ca"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'are', 'many', '<REpEat:5>', 'of', 'the', 'protagonists', 'it', 'is', 'abhored', 'it', '.', '.', '.', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ruleâ€‘Based POS(parts of speech) Tagger"
      ],
      "metadata": {
        "id": "jSi3SakCQG_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distinguish between nouns, adjectives and verbs only\n"
      ],
      "metadata": {
        "id": "q1UJajz5QO1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POS_tagger:\n",
        "  def __init__(self):\n",
        "    self.pronouns={\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"us\", \"them\"}\n",
        "    self.determiners = {\"the\", \"a\", \"an\", \"this\", \"that\", \"these\", \"those\",\"there\"}\n",
        "    self.adj_endings=['ous', 'ful', 'ive', 'al', 'ic', 'able', 'ible','ary']\n",
        "    self.verb_endings=['ing','ed','en','es','s','ise','ize']\n",
        "    self.be_verb={\"is\",\"am\",\"are\",\"was\",\"were\"}\n",
        "    self.adv_endings=['ly','ily']\n",
        "    self.adverbs_common={\"very\", \"most\" ,\"so\"}\n",
        "    self.noun_endings=['ment', 'ness', 'ity', 'tion', 'sion', 'er', 'or']\n",
        "    self.prepositions=[\"of\",\"to\",\"in\",\"for\",\"on\",\"with\",\"at\",\"by\",\"from\",\"about\",\"over\",\"after\",\"as\"]\n",
        "    self.pre={\"REpEat\"}\n",
        "  def tagger(self,tokens:list[str])->list[tuple[str,str]]:\n",
        "    tagging_done=[]\n",
        "    for token in tokens:\n",
        "      if re.fullmatch(r'[.,!?;:\\'\\\"()\\[\\]{}]', token):\n",
        "        tagger=\"punctuation\"\n",
        "      elif token in self.pronouns:\n",
        "        tagger=\"pronoun\"\n",
        "      elif token in self.determiners:\n",
        "        tagger=\"determiners\"\n",
        "      elif token in self.adverbs_common:\n",
        "        tagger=\"adverb\"\n",
        "      elif token in self.be_verb:\n",
        "        tagger=\"verb\"\n",
        "      elif token in self.prepositions:\n",
        "        tagger=\"preposition\"\n",
        "      elif re.fullmatch(r'\\d+(\\.\\d+)?', token):\n",
        "        tagger = \"NUM\"\n",
        "      elif token.startswith(\"<REPEAT\"):\n",
        "        tagger = \"OTHER\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.noun_endings) :\n",
        "        tagger=\"noun\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.adj_endings) :\n",
        "        tagger=\"adjective\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.adv_endings) :\n",
        "        tagger=\"adverb\"\n",
        "      elif any(token.endswith(suffix)for suffix in self.verb_endings) :\n",
        "        tagger=\"verb\"\n",
        "      elif any(token.startswith(pre)for pre in self.pre):\n",
        "        tagger=\"ignore\"\n",
        "      else:\n",
        "        tagger=\"other\"\n",
        "      tagging_done.append((token,tagger))\n",
        "    return tagging_done"
      ],
      "metadata": {
        "id": "tLTqVMBDQNKC"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tokenizer = customtokenizer()\n",
        "    tagger = POS_tagger()\n",
        "    sample_1 = \"Sooooooo scary!!!IT's very arduous!!\"\n",
        "    sample_2=\"there are mannnnny of the PROTAGONISTS who've abhored IT...!!\"\n",
        "    tokens = tokenizer.tokenize(sample_1)\n",
        "    tagging_done = tagger.tagger(tokens)\n",
        "    tokens = tokenizer.tokenize(sample_1)\n",
        "    print(tokens)\n",
        "for token, tagger in tagging_done:\n",
        "  if re.fullmatch(r'<REpEat:\\d+>', token):\n",
        "    continue\n",
        "  else:\n",
        "    print(f\"{token:15} : {tagger}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T431_uS5WTZX",
        "outputId": "34a6b06e-1493-41ed-9571-eaf7c9f7fe36"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['so', '<REpEat:7>', 'scary', '!', '!', '!', 'it', 'is', 'very', 'arduous', '!', '!']\n",
            "so              : adverb\n",
            "scary           : adjective\n",
            "!               : punctuation\n",
            "!               : punctuation\n",
            "!               : punctuation\n",
            "it              : pronoun\n",
            "is              : verb\n",
            "very            : adverb\n",
            "arduous         : adjective\n",
            "!               : punctuation\n",
            "!               : punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Stemmer or Lemmatizer"
      ],
      "metadata": {
        "id": "H-buL3dMf59f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The motivation of this pipeline is to reduce similar tokens like \"eaten\",\"ate\",\"eating\" to their stem word i.e.\"eat\". But it is to be taken care that over-stemming is avoided like \"protagonists\" isn't converted to \"protagon\". Basically I will try to convert only those tokens which are verb :)"
      ],
      "metadata": {
        "id": "2rTftbyigJPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class lemmatizer:\n",
        "  def __init__(self):\n",
        "    self.verb_endings=['ing','ed','en','es','s','ise','ize']\n",
        "    self.be_verb={\"is\",\"am\",\"are\",\"was\",\"were\"}\n",
        "    self.noun_endings=['ment', 'ness', 'ity', 'tion', 'sion', 'er', 'or']\n",
        "    self.adj_endings=['ous', 'ful', 'ive', 'al', 'ic', 'able', 'ible','ary']\n",
        "\n",
        "  def lemmatize(self,token:str,pos:str)->str:\n",
        "    lemma=token\n"
      ],
      "metadata": {
        "id": "fG0U3qIjf_8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}